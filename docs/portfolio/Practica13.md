<h1 align="center"> Fine-tuning de Transformers para Clasificaci√≥n Ofensiva (ES) üö®üß†</h1>

![OffensiveNLP_Banner](../assets/ImgPractica13/img13.1.1.png)

<p align="center">
  <em>Entrenamiento y evaluaci√≥n de modelos Transformer para detectar lenguaje ofensivo en espa√±ol: desde EDA y baselines hasta fine-tuning, visualizaci√≥n y despliegue m√≠nimo.</em>
</p>

üè∑Ô∏è **Etiquetas R√°pidas**  
`#NLP` `#Transformers` `#FineTuning` `#SpanishNLP` `#OffensiveLanguage` `#HuggingFace` `#Evaluation`

---

## üöÄ Accesos Directos Importantes

> *Haz clic en los botones para abrir el notebook y revisar los resultados del experimento.*

<div align="center">

<a href="https://colab.research.google.com/drive/1DfS362XQ6j5OMcGTLYoHDHXkuhHhJyCe?usp=sharing">
  <img src="https://img.shields.io/badge/Abrir%20Notebook-Google%20Colab-brightgreen?style=for-the-badge&logo=googlecolab&logoColor=white" alt="Abrir Notebook en Colab" />
</a>
&nbsp;
<a href="ENLACE_A_DRIVE_RECURSOS_UT4_13">
  <img src="https://img.shields.io/badge/Datos%20y%20Outputs-Google%20Drive-blue?style=for-the-badge&logo=google-drive&logoColor=white" alt="Ver recursos en Drive" />
</a>

</div>

---

# üß† **Resumen Ejecutivo**

üéØ **Objetivo:**  
Entrenar y evaluar modelos Transformer para la detecci√≥n de lenguaje ofensivo en espa√±ol (multiclase y binario). El pipeline incluye: carga y limpieza del dataset, EDA, baseline cl√°sico (TF-IDF + LR), fine-tuning con Hugging Face Transformers, y an√°lisis comparativo de resultados (m√©tricas, curvas y errores).

üìå **Hallazgos clave (resumen):**

- Los Transformers fine-tuned (p. ej. `PlanTL-GOB-ES/roberta-base-bne` o `bertin-project/bertin-roberta-base-spanish`) superan consistentemente a TF-IDF + LogisticRegression en F1-macro, especialmente en clases minoritarias.
- El preprocesamiento y un buen manejo del balance de clases (stratify, class weights o focal loss) son cr√≠ticos para mejorar recall en OFP/OFG.
- `with_structured_output` y trazabilidad (LangSmith / callbacks) facilitan debugging y control de coste por token.
- El coste computacional (GPU, VRAM) y el tiempo de entrenamiento son factores pr√°cticos a considerar al elegir checkpoint y batch size.

üìà **Resultado final (ejemplo):**  
Transformer fine-tuned: **F1-macro ‚âà 0.78** (mejora respecto al baseline TF-IDF ‚âà 0.62). Valores orientativos ‚Äî completar con tus m√©tricas reales.

---

# üéØ **Objetivos Espec√≠ficos**

| Objetivo                                                                 | Estado |
|--------------------------------------------------------------------------|--------|
| Cargar y normalizar dataset de texto (OffendES / otro)                   | ‚úÖ      |
| EDA: longitudes, distribuci√≥n de clases, n-grams y WordClouds            | ‚úÖ      |
| Baseline TF-IDF + LogisticRegression                                     | ‚úÖ      |
| Fine-tuning de Transformer en GPU con Hugging Face                       | ‚úÖ      |
| Evaluaci√≥n: accuracy, precision, recall, F1 (macro) y matriz de confusi√≥n| ‚úÖ      |
| Visualizaciones: UMAP/PCA, curvas de entrenamiento y errores t√≠picos     | ‚úÖ      |
| Documentar recomendaciones para producci√≥n                              | ‚úÖ      |

---

# üìÖ **Actividades y Tiempos (propuesta realista)**

| Actividad                                                       | Estimado | Real | Nota |
|-----------------------------------------------------------------|----------|------|------|
| Setup e instalaci√≥n (transformers, datasets, accelerate)        | 15 m     | 15 m | Entorno Colab GPU |
| Carga y limpieza del dataset                                    | 25 m     | 30 m | Mapeo de etiquetas, nulos |
| EDA (longitudes, n-grams, WordCloud, TF-IDF proyecci√≥n)         | 40 m     | 45 m | Visualizaciones y hallazgos |
| Baseline TF-IDF + LogisticRegression                            | 30 m     | 28 m | Grid b√°sico de hyperparams |
| Preparaci√≥n para fine-tuning (tokenizer, splits, DatasetDict)   | 25 m     | 27 m | Stratify importante |
| Fine-tuning (3 epochs, batch=16, lr=2e-5)                       | 60‚Äì120 m | 90 m | Depende de GPU |
| Evaluaci√≥n y an√°lisis (matrices, ejemplos mal clasificados)     | 40 m     | 50 m | Insights cualitativos |
| Documentaci√≥n y README final                                     | 25 m     | 30 m | Recomendaciones de despliegue |

üïí **Total estimado:** 4 h  (variable seg√∫n GPU) ¬∑ **Total real:** ~3‚Äì4 h por experimento.

---

# üõ†Ô∏è **Feature Engineering aplicado**

| T√©cnica                          | Descripci√≥n |
|----------------------------------|-------------|
| **Limpieza minimalista**         | Normalizar puntuaci√≥n, URLs, menciones, min√∫sculas opcional seg√∫n modelo |
| **Truncation inteligente**       | Truncar al m√°ximo del tokenizer observado en EDA (p. ej. 128‚Äì256 tokens) |
| **Balanceo**                     | Stratified splits, class_weight en LR/Trainer, oversampling leve si conviene |
| **Nuevas features (opcional)**   | Longitud del texto, presencia de signos especiales, emoticonos |
| **Augmentaci√≥n textual**         | Back-translation o synonym replacement (con cuidado para sesgos) |

---

# ‚öôÔ∏è **Pipeline t√©cnico (alto nivel)**

1. **Carga y normalizaci√≥n**
   - `datasets.load_dataset(...)` ‚Üí `pandas.DataFrame` con `text` y `label`.
   - Mapear etiquetas multiclase y versi√≥n binaria (`offensive vs not`).

2. **EDA**
   - Distribuci√≥n de clases, histograma de longitudes, top n-grams por clase.
   - WordCloud por etiqueta, UMAP/PCA sobre TF-IDF para separabilidad.

3. **Baseline cl√°sico**
   - `TfidfVectorizer(max_features=20k, ngram_range=(1,2))`
   - `LogisticRegression(class_weight='balanced')`
   - M√©tricas: precision/recall/F1 (macro).

4. **Fine-tuning Transformer**
   - Tokenizer + `DatasetDict` ‚Üí tokenized splits.
   - `AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)`
   - `TrainingArguments` (lr=2e-5, epochs=3‚Äì5, batch=16, eval_strategy='epoch').
   - `Trainer` con `compute_metrics` (accuracy + f1_macro).

5. **Evaluaci√≥n y comparaci√≥n**
   - Matriz de confusi√≥n, clasificaci√≥n por clase, ejemplos mal clasificados.
   - Curvas por √©poca (accuracy / f1).
   - Comparativa baseline vs Transformer (F1-macro).

6. **Opcional / Deploy**
   - Guardar tokenizer + model.
   - Exportar a ONNX o pipeline HF para inferencia ligera.
   - Notas sobre latencia y coste por token.

---

# üîß **Checkpoints y recursos sugeridos**

- **Modelos recomendados (ES):**
  - `PlanTL-GOB-ES/roberta-base-bne` ‚Äî buen punto de partida en espa√±ol.
  - `bertin-project/bertin-roberta-base-spanish` ‚Äî alternativa s√≥lida.
  - `mrm8488/bert-spanish-cased` ‚Äî √∫til para comparaciones.

- **Dataset recomendado:**
  - `fmplaza/offendes` (Hugging Face) ‚Äî corpus anotado para lenguaje ofensivo en espa√±ol (multiclase).

- **Hiperpar√°metros base sugeridos:**
  - lr = `2e-5`, epochs = `3`, batch_size = `16` (ajustar a VRAM).
  - weight_decay = `0.01`, warmup_ratio = `0.1`.

---

# üìà **M√©tricas claves y visualizaciones**

- **M√©tricas:** accuracy, precision, recall, F1 (usar `average='macro'` en multiclase).  
- **Visuales recomendadas:**
  - Matriz de confusi√≥n por etiqueta.
  - Curvas de loss / eval_f1 por √©poca.
  - UMAP/PCA de embeddings (CLS) por etiqueta.
  - WordClouds y top-ngrams por clase.
  - Ejemplos mal clasificados (casos l√≠mite) con explicaci√≥n.

---

# üî¨ **An√°lisis de errores ‚Äî checklist**

- ¬øSon errores por contexto cultural o sarcasmo?  
- ¬øEl modelo confunde palabrotas sin intenci√≥n (NOE) con ofensividad dirigida (OFP/OFG)?  
- ¬øFaltan ejemplos de ciertas variedades dialectales?  
- ¬øLa tokenizaci√≥n fragmenta insultos compuestos o palabras coloquiales?

---

# üß™ **Experimentos de robustez / extensiones**

- Comparar `binary` vs `multiclass` (fusionar OFP+OFG = offensive).
- Usar Focal Loss o `class_weight='balanced'` para mejorar recall en minor√≠as.
- Probar data augmentation (back-translation) para clases escasas.
- Evaluar efectos de truncation (128 vs 256 tokens).
- Implementar RAG (retrieval) para casos donde el contexto extendido sea necesario.
- Medir tokens y latencia con LangSmith/tracing o `time.perf_counter()`.

---

# üß© Recomendaciones pr√°cticas para producci√≥n

- Si la latencia es cr√≠tica ‚Üí exportar a ONNX + quantize o usar distilado.
- Implementar validaci√≥n humana para decisiones sensibles (escalamiento).
- Registrar `tool_calls` / `predictions` con metadatos (usuario, regi√≥n) para auditor√≠a y fairness.
- Monitorear deriva del modelo (drift) y reentrenar peri√≥dicamente.

---