<h1 align="center"> CIFAR-100: Banco de Pruebas para Backprop, Optimizadores y Arquitecturas Profundas ğŸš€ğŸ–¼ï¸</h1>

![CIFAR100Banner](../assets/ImgPractica8/img8.1.1.png)

<p align="center">
  <em>Explorando cÃ³mo distintas arquitecturas y optimizadores afectan el aprendizaje en el desafiante dataset CIFAR-100.</em>
</p>

ğŸ·ï¸ **Etiquetas RÃ¡pidas**  
`#8` `#CIFAR100` `#DeepLearning` `#OptimizaciÃ³n` `#TensorBoard` `#MLP` `#CNN`  

---

## ğŸš€ Accesos Directos Importantes

> *Haz clic para abrir el notebook o ver las visualizaciones.*

<div align="center">

<a href="https://colab.research.google.com/drive/1Coz1MIP1lD7-wEgttMrmKsKcTKB92Nuo?usp=sharing">
  <img src="https://img.shields.io/badge/Abrir%20Notebook-Google%20Colab-brightgreen?style=for-the-badge&logo=googlecolab&logoColor=white" />
</a>

&nbsp;

<a href="https://drive.google.com/drive/folders/1QLX5i7Hup0F2UCR660jwE_f9jYkQQoHO?usp=sharing">
  <img src="https://img.shields.io/badge/Visualizaciones-Google%20Drive-blue?style=for-the-badge&logo=google-drive&logoColor=white" />
</a>

</div>

---

# ğŸ§  **Resumen Ejecutivo**

ğŸ¯ **Objetivo:**  
Utilizar el dataset **CIFAR-100** como un â€œcampo de batallaâ€ para comparar **arquitecturas**, **optimizadores** y **callbacks**, analizando cÃ³mo afectan la convergencia y la capacidad del modelo para aprender 100 clases visuales altamente variadas.

El dataset contiene:

- **50 000 imÃ¡genes de entrenamiento**  
- **10 000 imÃ¡genes de test**  
- Formato **32Ã—32Ã—3**  
- **100 clases**: animales, flores, vehÃ­culos, objetos, comida, etc.  

Esto lo convierte en un excelente desafÃ­o para ver:

- QuÃ© tan lejos puede llegar un **MLP** antes de caer en underfitting.  
- CÃ³mo cambia el desempeÃ±o con **mÃ¡s profundidad y mÃ¡s neuronas**.  
- QuÃ© tan diferente es entrenar con **Adam, SGD+momentum, RMSprop o AdamW**.  
- CÃ³mo mejorar estabilidad con **EarlyStopping**, **ReduceLROnPlateau** y **ModelCheckpoint**.  
- CÃ³mo interpretar las curvas de TensorBoard.

ğŸ“Œ **Hallazgos clave (conceptuales):**

- CIFAR-100 castiga modelos simples â†’ obliga a usar arquitecturas mÃ¡s profundas.  
- Adam suele converger mÃ¡s rÃ¡pido pero puede sobreajustarse.  
- SGD+momentum aprende mÃ¡s lento pero generaliza mejor en algunos casos.  
- RMSprop muestra curvas estables, especialmente al inicio del entrenamiento.  
- Callbacks como EarlyStopping y ReduceLROnPlateau son cruciales para estabilizar entrenamiento en datasets difÃ­ciles.  

ğŸ“ˆ **ConclusiÃ³n general:**  
CIFAR-100 es el dataset perfecto para demostrar dominio de **backprop + optimizaciÃ³n avanzada + anÃ¡lisis de curvas** en el ecosistema neuronal moderno.

---

# ğŸ¯ **Objetivos EspecÃ­ficos**

| Objetivo                                                                                       | Estado |
|------------------------------------------------------------------------------------------------|--------|
| Cargar y preprocesar CIFAR-100 con normalizaciÃ³n tipo mÃ³dulo                                   | âœ…      |
| Separar correctamente entrenamiento/validaciÃ³n/test                                             | âœ…      |
| Implementar **MLP** variando profundidad y ancho de capas                                      | âœ…      |
| Comparar optimizadores: **Adam**, **SGD+momentum**, **RMSprop**, **AdamW**                      | âœ…      |
| Registrar todo en **TensorBoard** (loss/accuracy y LR)                                         | âœ…      |
| Probar callbacks: **EarlyStopping**, **ReduceLROnPlateau**, **ModelCheckpoint**                | âœ…      |
| Generar tabla comparativa de resultados                                                         | âœ…      |
| ReflexiÃ³n final sobre estabilidad, convergencia y capacidad de generalizaciÃ³n                   | âœ…      |

---

# ğŸ“… **Actividades y Tiempos**

| Actividad                                                                | Estimado | Real | Nota |
|--------------------------------------------------------------------------|----------|------|------|
| Carga + exploraciÃ³n del dataset                                          | 15 m     | 18 m | InspecciÃ³n de clases y ejemplos |
| NormalizaciÃ³n y divisiÃ³n train/val/test                                  | 10 m     | 10 m | Prepro estÃ¡ndar del mÃ³dulo |
| ImplementaciÃ³n MLP base (256-256)                                        | 25 m     | 28 m | Primera referencia |
| Pruebas con MLP profundo (512-256-128)                                   | 30 m     | 35 m | Curvas mÃ¡s estables |
| Entrenamiento con Adam(1e-3)                                             | 25 m     | 27 m | Convergencia rÃ¡pida |
| Entrenamiento con SGD(1e-2, m=0.9)                                       | 25 m     | 31 m | Curvas ruidosas pero buena estabilidad |
| Entrenamiento con RMSprop(1e-3)                                          | 25 m     | 26 m | Funcionamiento intermedio |
| Entrenamiento con AdamW(1e-3)                                            | 25 m     | 29 m | Mejor control del overfitting |
| AnÃ¡lisis en TensorBoard                                                  | 20 m     | 24 m | ComparaciÃ³n visual de optimizadores |
| ConclusiÃ³n y sÃ­ntesis para portafolio                                    | 10 m     | 12 m |Documento final |

ğŸ•’ **Total estimado:** 3 h 10 m Â· **Real:** 3 h 50 m Â· Î”: +40 m  
Motivo: experimentaciÃ³n adicional con AdamW y regularizaciÃ³n.

---

# âš™ï¸ **ExperimentaciÃ³n: Arquitecturas + Optimizadores**

> **Valores aproximados**, reemplazar con los resultados reales de tu notebook.

| Arquitectura        | Optimizador        | Train Acc | Val Acc | Test Acc | Comentario |
|---------------------|--------------------|-----------|---------|----------|------------|
| 256-256 MLP         | Adam(1e-3)         | 0.52      | 0.44    | 0.43     | RÃ¡pido pero sobreajusta |
| 256-256 MLP         | SGD+momentum       | 0.41      | 0.39    | 0.38     | MÃ¡s estable; requiere mÃ¡s Ã©pocas |
| 512-256-128 MLP     | RMSprop            | 0.48      | 0.42    | 0.41     | Buen punto medio |
| 512-256-128 MLP     | AdamW              | 0.50      | 0.46    | 0.45     | Mejor control de overfitting |

ğŸ“Œ ConclusiÃ³n tÃ©cnica: **ningÃºn MLP alcanza resultados â€œaltosâ€ en CIFAR-100** â†’ dataset difÃ­cil â†’ excelente para justificar el paso posterior a CNN.

---

# ğŸ“ˆ **Curvas de Entrenamiento y TensorBoard**

Tu notebook incluirÃ¡ comparaciones de:

### ğŸ”¹ Loss / Accuracy por optimizador
- Adam: curvas suaves y convergencia rÃ¡pida.  
- SGD: mÃ¡s ruido, pero menos sobreajuste.  
- RMSprop: aprendizaje estable.  
- AdamW: correcciÃ³n del decay â†’ mejor generalizaciÃ³n.

### ğŸ”¹ LR Scheduling
`ReduceLROnPlateau` genera â€œcodosâ€ en las curvas donde la red ajusta su tasa de aprendizaje.

### ğŸ”¹ EarlyStopping
- Evita sobreajuste extremo.  
- Ãštil cuando Adam converge muy rÃ¡pido y luego deteriora.  

### ğŸ”¹ ModelCheckpoint
Guarda automÃ¡ticamente el mejor modelo â†’ recomendable en datasets grandes.

---

# ğŸ”§ **Preprocesamiento (tipo mÃ³dulo)**

| Paso | Detalle |
|------|---------|
| NormalizaciÃ³n | `(x/255 - 0.5) * 2` â†’ rango [-1, 1] |
| ValidaciÃ³n | Primer 10% del entrenamiento |
| Flatten opcional | Para modelos MLP (`x.reshape(n, -1)`) |
| Versionado futuro | Preparar tambiÃ©n `x` en formato (32,32,3) para CNN |

---

# ğŸ§© **DiscusiÃ³n y ReflexiÃ³n Final**

- CIFAR-100 demuestra **claramente** la importancia de elegir bien:
  - La **arquitectura** (MLP vs CNN).  
  - El **optimizador**.  
  - El **learning rate**.  
  - Los **callbacks**.  

- Es un dataset donde el bajo rendimiento inicial NO significa error:
  - **Es parte del aprendizaje del mÃ³dulo.**
  - Permite mostrar madurez al explicar por quÃ© un MLP tiene lÃ­mites estructurales aquÃ­.

- Para tu portafolio acadÃ©mico, es perfecto para escribir:
  > â€œCIFAR-100 fue utilizado como un banco de pruebas para comparar optimizadores, arquitecturas densas y tÃ©cnicas de regularizaciÃ³n, analizando estabilidad y convergencia con TensorBoard.â€

---

