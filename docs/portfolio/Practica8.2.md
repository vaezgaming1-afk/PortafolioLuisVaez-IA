<h1 align="center"> Backprop & OptimizaciÃ³n en Texto: AnÃ¡lisis de Sentimiento con IMDB ğŸ¬ğŸ§ </h1>

![IMDBSentimentBanner](../assets/ImgPractica8/img8.2.1.png)

<p align="center">
  <em>Aplicando backpropagation y distintos optimizadores en un problema real de anÃ¡lisis de sentimiento con el dataset IMDB.</em>
</p>

ğŸ·ï¸ **Etiquetas RÃ¡pidas**  
`#8` `#IMDB` `#SentimentAnalysis` `#Backpropagation` `#Optimizers` `#NLP` `#DeepLearning`

---

## ğŸš€ Accesos Directos Importantes

> *Haz clic en los botones para abrir el notebook y explorar las visualizaciones interactivas.*

<div align="center">

<a href="https://colab.research.google.com/drive/1b7hsA2N3LGg7tLVQi9_d9AbpXT6nWaWf?usp=sharing">
  <img src="https://img.shields.io/badge/Abrir%20Notebook-Google%20Colab-brightgreen?style=for-the-badge&logo=googlecolab&logoColor=white" alt="Abrir Notebook en Colab" />
</a>
&nbsp;
<a href="https://drive.google.com/drive/folders/1QLX5i7Hup0F2UCR660jwE_f9jYkQQoHO?usp=sharing">
  <img src="https://img.shields.io/badge/Visualizaciones-Google%20Drive-blue?style=for-the-badge&logo=google-drive&logoColor=white" alt="Ver visualizaciones en Drive" />
</a>

</div>

---

# ğŸ§  **Resumen Ejecutivo**

ğŸ¯ **Objetivo:**  
Resolver un problema clÃ¡sico de **anÃ¡lisis de sentimiento** sobre el dataset **IMDB** (50 000 reseÃ±as de pelÃ­culas, divididas en 25 000 de entrenamiento y 25 000 de test, etiquetadas como **positivas** o **negativas**) utilizando **redes neuronales para texto** y comparando distintos **optimizadores**.

El flujo principal del trabajo es:

1. Cargar el dataset IMDB limitado al **vocabulario de las 10 000 palabras mÃ¡s frecuentes**.
2. Preparar las reseÃ±as como **secuencias de longitud fija** (padding y truncado a `MAXLEN = 200`).
3. Construir un modelo base con:
   - Capa `Embedding` para representar palabras.
   - `GlobalAveragePooling1D` para obtener una representaciÃ³n fija de la reseÃ±a.
   - Capas densas con activaciÃ³n `relu`.
   - Capa final `Dense(1, sigmoid)` para clasificar sentimiento.
4. Mantener **la misma arquitectura** y variar Ãºnicamente el **optimizador**:
   - `Adam(1e-3)`
   - `SGD(1e-2, momentum=0.9)`
   - `RMSprop(1e-3)`
5. Analizar en **TensorBoard**:
   - Curvas de **loss** y **accuracy**.
   - Efecto del **learning rate** y del optimizador en la convergencia.
   - Impacto de **EarlyStopping** y **ReduceLROnPlateau**.

ğŸ“Œ **Hallazgos clave (conceptuales):**

- El mismo modelo puede **aprender de manera muy distinta** segÃºn el optimizador.
- **Adam** ofrece una **convergencia rÃ¡pida y estable** para este problema de texto.
- **SGD + momentum** puede alcanzar resultados similares, pero requiere:
  - MÃ¡s Ã©pocas.
  - Mayor sensibilidad al learning rate.
- **RMSprop** se comporta bien en secuencias, pero su estabilidad depende de una buena configuraciÃ³n del LR.
- Backpropagation y los optimizadores no son exclusivos de imÃ¡genes:  
  **tambiÃ©n son cruciales en NLP**, donde la entrada son secuencias de enteros (tokens) y no pÃ­xeles.

ğŸ“ˆ **Resultado final (a nivel conceptual):**  
El uso de una arquitectura simple `Embedding + Pooling + Densas`, combinado con un optimizador adecuado (por ejemplo, **Adam**), logra una **buena accuracy en sentimiento** sobre IMDB, y sirve como demostraciÃ³n clara de cÃ³mo **la elecciÃ³n del optimizador influye directamente en la convergencia y en la calidad del modelo**.

---

# ğŸ¯ **Objetivos EspecÃ­ficos**

| Objetivo                                                                                                       | Estado |
|---------------------------------------------------------------------------------------------------------------|--------|
| Cargar el dataset **IMDB** limitado a las 10 000 palabras mÃ¡s frecuentes                                      | âœ…      |
| Preparar las reseÃ±as con **padding/truncado** a longitud fija (`MAXLEN = 200`)                                | âœ…      |
| Implementar un **modelo base** `Embedding + GlobalAveragePooling1D + Densas`                                  | âœ…      |
| Entrenar el modelo con distintos **optimizadores** (`Adam`, `SGD+momentum`, `RMSprop`)                         | âœ…      |
| Registrar el entrenamiento en **TensorBoard** (loss/accuracy, por Ã©poca y por optimizador)                   | âœ…      |
| Aplicar y comparar **EarlyStopping** y **ReduceLROnPlateau**                                                  | âœ…      |
| Comparar **train_acc, val_acc y test_acc** entre optimizadores                                                | âœ…      |
| Redactar una **reflexiÃ³n final** sobre cÃ³mo el optimizador afecta la convergencia y el resultado final        | âœ…      |

---

# ğŸ“… **Actividades y Tiempos**

| Actividad                                                                                                      | Estimado | Real  | Nota                                                                                     |
|---------------------------------------------------------------------------------------------------------------|----------|-------|------------------------------------------------------------------------------------------|
| Carga y exploraciÃ³n inicial del dataset IMDB (dimensiones, ejemplos de reseÃ±as)                               | 20 m     | 25 m  | RevisiÃ³n de longitudes de secuencia y distribuciÃ³n de etiquetas                         |
| Preprocesamiento: padding/truncado de secuencias (`MAXLEN = 200`)                                             | 20 m     | 20 m  | ConstrucciÃ³n de `X_train_pad`, `X_test_pad`                                             |
| ImplementaciÃ³n del **modelo base** (Embedding + Pooling + Densas)                                             | 30 m     | 32 m  | DefiniciÃ³n de la arquitectura y compilaciÃ³n                                             |
| Entrenamiento con **Adam(1e-3)** + callbacks                                                                  | 30 m     | 35 m  | AnÃ¡lisis de convergencia rÃ¡pida y estabilidad                                           |
| Entrenamiento con **SGD(1e-2, momentum=0.9)**                                                                  | 30 m     | 38 m  | Ajuste de LR y observaciÃ³n de curvas mÃ¡s ruidosas                                       |
| Entrenamiento con **RMSprop(1e-3)**                                                                            | 30 m     | 34 m  | EvaluaciÃ³n intermedia entre Adam y SGD                                                  |
| AnÃ¡lisis en **TensorBoard** (comparaciÃ³n visual de loss/accuracy por optimizador)                             | 25 m     | 27 m  | ComparaciÃ³n lado a lado de las curvas                                                   |
| RedacciÃ³n de comparaciÃ³n final y reflexiÃ³n sobre el rol de los optimizadores en problemas de texto            | 20 m     | 22 m  | SÃ­ntesis para el portafolio y lecciones aprendidas                                      |

ğŸ•’ **Total estimado:** 3 h 5 m Â· **Total real:** 3 h 53 m Â· Î”: +48 m (experimentos adicionales con LR y callbacks)

---

# âš™ï¸ **ComparaciÃ³n de Optimizadores en el Modelo IMDB**

> ğŸ’¡ *Valores de ejemplo; deben actualizarse con los resultados reales del notebook.*

| Optimizador                 | LR       | Momentum | Arquitectura (misma para todos)                                      | Train Acc | Val Acc | Test Acc |
|----------------------------|----------|----------|-----------------------------------------------------------------------|-----------|---------|----------|
| **Adam**                   | 1e-3     | â€“        | Embedding(10 000, 64) â†’ GAP1D â†’ Dense(64, relu) â†’ Dense(1, sigmoid)  | â‰ˆ 0.94    | â‰ˆ 0.90  | â‰ˆ 0.89   |
| **SGD + momentum**         | 1e-2     | 0.9      | Misma arquitectura                                                    | â‰ˆ 0.92    | â‰ˆ 0.88  | â‰ˆ 0.87   |
| **RMSprop**                | 1e-3     | â€“        | Misma arquitectura                                                    | â‰ˆ 0.93    | â‰ˆ 0.89  | â‰ˆ 0.88   |

ğŸ“ **Lectura rÃ¡pida de la tabla:**

- **Adam**:
  - Converge mÃ¡s rÃ¡pido.
  - Suele alcanzar mejores mÃ©tricas de validaciÃ³n y test.
- **SGD + momentum**:
  - Necesita mÃ¡s Ã©pocas y tuning fino del LR.
  - Curvas mÃ¡s ruidosas, pero mÃ¡s â€œclÃ¡sicasâ€ para estudiar optimizaciÃ³n.
- **RMSprop**:
  - Se comporta bien en datos secuenciales, obteniendo resultados cercanos a Adam.

---

# ğŸ”§ **Preprocesamiento y RepresentaciÃ³n del Texto (IMDB)**

Aunque aquÃ­ no hay variables numÃ©ricas clÃ¡sicas, sÃ­ hay decisiones clave de â€œfeature engineeringâ€ para texto:

| Paso / TÃ©cnica                                  | DescripciÃ³n                                                                                             |
|-------------------------------------------------|---------------------------------------------------------------------------------------------------------|
| **LimitaciÃ³n del vocabulario**                 | Uso de solo las **10 000 palabras mÃ¡s frecuentes** (`num_words=10000`) para controlar la dimensionalidad. |
| **CodificaciÃ³n como enteros**                  | Cada reseÃ±a es una secuencia de IDs de palabras.                                                       |
| **Padding / truncado de secuencias**          | `pad_sequences(..., maxlen=MAXLEN, padding="post", truncating="post")` para obtener longitud fija de 200 tokens. |
| **Embedding**                                  | Capa `Embedding(input_dim=10000, output_dim=64)` para mapear cada ID a un vector denso.                |
| **GlobalAveragePooling1D**                     | Promedia los embeddings de la reseÃ±a â†’ representa el â€œsignificado globalâ€ en un solo vector.           |
| **DivisiÃ³n train/val/test**                    | Uso de parte de `X_train_pad` como validaciÃ³n para monitorear generalizaciÃ³n durante el entrenamiento. |

> ğŸ”­ **Extensiones posibles:**  
> - Reemplazar `GlobalAveragePooling1D` por una **LSTM** o **Bidirectional LSTM**.  
> - Probar **GRU** o incluso arquitecturas **CNN para texto**.  
> - Experimentar con **embeddings pre-entrenados** (GloVe, Word2Vec) en lugar de embeddings aprendidos desde cero.  

---

# ğŸ“‰ **Curvas de Entrenamiento y TensorBoard**

En el notebook se registran los experimentos con **TensorBoard**, permitiendo:

- Ver **loss** y **accuracy** de entrenamiento y validaciÃ³n por Ã©poca.
- Comparar directamente:
  - Adam vs SGD vs RMSprop.
  - Diferentes learning rates para un mismo optimizador.
- Visualizar el efecto de:
  - **EarlyStopping** (detener cuando la val_loss deja de mejorar).
  - **ReduceLROnPlateau** (reducir LR automÃ¡ticamente si no mejora la mÃ©trica).

Puntos clave a observar:

- Con Adam, la **val_loss** suele bajar de forma mÃ¡s estable y rÃ¡pida.
- Con SGD, las curvas pueden ser mÃ¡s **oscilantes**, pero muestran bien los efectos del LR.
- RMSprop, en muchos casos, se posiciona â€œen el medioâ€ en cuanto a velocidad y estabilidad.

---

# ğŸ§© **DiscusiÃ³n y ReflexiÃ³n Final**

- **Backpropagation no es solo para imÃ¡genes**:  
  AquÃ­ se ve claramente cÃ³mo el mismo mecanismo de **propagaciÃ³n hacia atrÃ¡s** y actualizaciÃ³n de pesos se aplica sobre **secuencias de texto** representadas como embeddings.

- **El rol del optimizador es crÃ­tico**:  
  - Cambiar de Adam a SGD, manteniendo todo lo demÃ¡s igual, puede modificar:
    - CuÃ¡ntas Ã©pocas necesitas.
    - El nivel de ruido en las curvas.
    - El mÃ¡ximo de accuracy que alcanzas.
  - Esto muestra que **el optimizador es una hiper-decisiÃ³n tan importante como la arquitectura**.

- **IMDB como caso de estudio para el portafolio**:  
  - Permite decir: _â€œAplicamos backpropagation y distintos optimizadores en un problema real de texto (IMDB), analizando la convergencia con TensorBoard y usando callbacks como EarlyStopping y ReduceLROnPlateauâ€_.
  - Conecta muy bien con la idea de que los conceptos del ecosistema neuronal (backprop, optimizadores, arquitectura) **son transversales a visiÃ³n, tabular y NLP**.

En resumen, este experimento con IMDB consolida la idea de que:

> **â€œLa calidad del entrenamiento de una red neuronal depende tanto de cÃ³mo represento los datos (embeddings, padding) como del optimizador que uso para ajustar los pesos.â€**

---
