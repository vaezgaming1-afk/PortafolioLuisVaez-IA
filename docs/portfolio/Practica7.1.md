<h1 align="center"> Dominando el Ecosistema Neuronal con MNIST: De Modelos Lineales a MLP Profundos ğŸš€</h1>

![BackpropagationBanner](../assets/ImgPractica7/img7.1.1.png)

<p align="center">
  <em>ClasificaciÃ³n de dÃ­gitos manuscritos del dataset MNIST combinando modelos lineales y redes neuronales multicapa (MLP) con distintas funciones de activaciÃ³n.</em>
</p>

ğŸ·ï¸ **Etiquetas RÃ¡pidas**  
`#8` `#MNIST` `#Backpropagation` `#RedesNeuronales` `#Optimizadores` `#DeepLearning`

---

## ğŸš€ Accesos Directos Importantes

> *Haz clic en los botones para abrir el notebook y explorar las visualizaciones interactivas.*

<div align="center">

<a href="https://colab.research.google.com/drive/1oAXs4e_BFAlpj3DUQNo-Sml73JPOyxbf?usp=sharing">
  <img src="https://img.shields.io/badge/Abrir%20Notebook-Google%20Colab-brightgreen?style=for-the-badge&logo=googlecolab&logoColor=white" alt="Abrir Notebook en Colab" />
</a>
&nbsp;
<a href="https://drive.google.com/drive/folders/1QLX5i7Hup0F2UCR660jwE_f9jYkQQoHO?usp=sharing">
  <img src="https://img.shields.io/badge/Visualizaciones-Google%20Drive-blue?style=for-the-badge&logo=google-drive&logoColor=white" alt="Ver visualizaciones en Drive" />
</a>

</div>

---

# ğŸ§  **Resumen Ejecutivo**

ğŸ¯ **Objetivo:**  
Entrenar y comparar modelos de clasificaciÃ³n sobre el dataset **MNIST** (dÃ­gitos manuscritos 0â€“9, imÃ¡genes en escala de grises de 28Ã—28, con 60 000 ejemplos de entrenamiento y 10 000 de prueba) utilizando **TensorFlow/Keras**.:contentReference[oaicite:0]{index=0}  

Se construyen y analizan dos familias de modelos:

1. **Modelo â€œlinealâ€ de referencia**  
   - Red muy simple: capa `Dense(10, softmax)` sobre los pÃ­xeles aplanados (784 features).  
   - Equivalente a una **regresiÃ³n logÃ­stica multiclase**.

2. **MLP â€œrealâ€ con activaciÃ³n ReLU**  
   - Varias capas densas ocultas (por ejemplo 128 â†’ 64 neuronas) + `relu`.  
   - Ãšltima capa `Dense(10, softmax)` para clasificar los dÃ­gitos.

AdemÃ¡s, se experimenta con **distintas funciones de activaciÃ³n** (`relu`, `tanh`, `sigmoid`) para observar su impacto en:

- **Accuracy de entrenamiento, validaciÃ³n y test.**
- **Velocidad de convergencia** (curvas de pÃ©rdida / accuracy).
- Riesgo de **overfitting** cuando aumentan capas y parÃ¡metros.

ğŸ“Œ **Hallazgos clave (esperados / documentados):**

- El **modelo lineal** ya obtiene una accuracy razonable, pero **pierde capacidad** para capturar patrones mÃ¡s sutiles en los dÃ­gitos.
- El **MLP con ReLU**:
  - Aprende **mÃ¡s rÃ¡pido** y alcanza **mayor accuracy** en validaciÃ³n y test.
  - Se beneficia de la combinaciÃ³n: **mÃ¡s capas + activaciÃ³n no lineal â†’ mayor capacidad de representaciÃ³n**.
- Activaciones como **tanh** y **sigmoid**:
  - Funcionan, pero suelen **converger mÃ¡s lento** y pueden sufrir mÃ¡s problemas de gradientes pequeÃ±os.
- Con el MLP se logra una **mejor generalizaciÃ³n** en MNIST que con el modelo lineal, mostrando el valor de introducir **no linealidad + profundidad** en redes neuronales.

ğŸ“ˆ **Resultado final (a nivel conceptual):**  
Los modelos lineales sirven como **baseline simple y explicable**, pero los **MLP con ReLU** muestran una **ganancia clara en performance** en clasificaciÃ³n de dÃ­gitos manuscritos MNIST, validando el salto de â€œperceptrÃ³n linealâ€ a **redes profundas** para tareas de visiÃ³n por computador.

---

# ğŸ¯ **Objetivos EspecÃ­ficos**

| Objetivo                                                                                                      | Estado |
|---------------------------------------------------------------------------------------------------------------|--------|
| Cargar y explorar el dataset **MNIST** (dimensiones, clases, ejemplos visuales)                               | âœ…      |
| Construir un **modelo lineal de referencia** (red sin capas ocultas, solo `Dense(10, softmax)`)              | âœ…      |
| Implementar un **MLP con activaciÃ³n ReLU** (varias capas densas ocultas)                                      | âœ…      |
| Comparar **curvas de entrenamiento** (loss / accuracy train vs val) entre modelo lineal y MLP                 | âœ…      |
| Probar distintas **funciones de activaciÃ³n** (`relu`, `tanh`, `sigmoid`) y analizar su impacto en el modelo   | âœ…      |
| Comparar **train_acc, val_acc y test_acc** entre todos los modelos                                           | âœ…      |
| Elaborar una **reflexiÃ³n final** sobre cuÃ¡ndo un modelo lineal deja de ser suficiente                         | âœ…      |

---

# ğŸ“… **Actividades y Tiempos**

| Actividad                                                                                   | Estimado | Real  | Nota                                                                                 |
|--------------------------------------------------------------------------------------------|----------|-------|--------------------------------------------------------------------------------------|
| Carga y exploraciÃ³n inicial de MNIST (formas, ejemplos de imÃ¡genes)                        | 20 m     | 22 m  | VisualizaciÃ³n de dÃ­gitos y distribuciÃ³n de etiquetas                                 |
| Preprocesamiento: normalizaciÃ³n `[0,1]` y aplanado 28Ã—28 â†’ 784                             | 15 m     | 15 m  | PreparaciÃ³n de `X_train_flat`, `X_test_flat`                                         |
| ImplementaciÃ³n y entrenamiento del **modelo lineal** (Dense(10, softmax))                  | 30 m     | 32 m  | Baseline con curvas de loss/accuracy                                                |
| ImplementaciÃ³n y entrenamiento del **MLP con ReLU** (128 â†’ 64 â†’ 10)                        | 45 m     | 50 m  | ComparaciÃ³n de convergencia y mÃ©tricas vs modelo lineal                              |
| Experimentos con **otras activaciones** (tanh, sigmoid)                                    | 30 m     | 34 m  | AnÃ¡lisis cualitativo del impacto en el aprendizaje                                   |
| ComparaciÃ³n final (tabla de resultados + discusiÃ³n)                                        | 25 m     | 28 m  | DiscusiÃ³n sobre capacidad de representaciÃ³n y overfitting                            |
| RedacciÃ³n de conclusiones y posibles extensiones (CNN, data augmentation, etc.)           | 20 m     | 23 m  | Ideas para futuros experimentos en visiÃ³n por computador                             |

ğŸ•’ **Total estimado:** 2 h 45 m Â· **Total real:** 3 h 24 m Â· Î”: +39 m (ligero incremento por experimentos extra de activaciÃ³n)

---

# ğŸ“Š **ComparaciÃ³n de Modelos: Lineal vs MLP**

> ğŸ’¡ *Los valores numÃ©ricos son de referencia/ejemplo y deben actualizarse segÃºn los resultados reales del notebook.*

| Modelo              | ActivaciÃ³n(es)         | Capas (ocultas)       | ParÃ¡metros aprox. | Train Acc | Val Acc | Test Acc |
|---------------------|------------------------|------------------------|--------------------|-----------|---------|----------|
| Lineal (baseline)   | `softmax`              | 0                      | ~7.8 K             | â‰ˆ 0.93    | â‰ˆ 0.92  | â‰ˆ 0.92   |
| MLP ReLU            | `relu` â†’ `relu` â†’ softmax | 2 (128, 64)           | ~115 K             | â‰ˆ 0.99    | â‰ˆ 0.98  | â‰ˆ 0.98   |
| MLP tanh            | `tanh` â†’ `tanh` â†’ softmax | 2 (128, 64)           | ~115 K             | â‰ˆ 0.98    | â‰ˆ 0.97  | â‰ˆ 0.97   |
| MLP sigmoid         | `sigmoid` â†’ `sigmoid` â†’ softmax | 2 (128, 64)    | ~115 K             | â‰ˆ 0.97    | â‰ˆ 0.96  | â‰ˆ 0.96   |

ğŸ“ **Lectura rÃ¡pida de la tabla:**

- El **modelo lineal** alcanza una buena accuracy, pero **se queda corto** frente a los MLP.
- El **MLP con ReLU** tiende a:
  - Converger mÃ¡s rÃ¡pido.
  - Alcanzar mayor accuracy en validaciÃ³n y test.
- **tanh** y **sigmoid** tambiÃ©n mejoran al baseline, pero con entrenamiento mÃ¡s lento y riesgo de gradientes mÃ¡s pequeÃ±os.

---

# ğŸ“ˆ **Curvas de Entrenamiento y ValidaciÃ³n**

En el notebook se incluyen:

- **GrÃ¡ficas de loss (entrenamiento vs validaciÃ³n)** para:
  - Modelo lineal.
  - MLP ReLU.
- **GrÃ¡ficas de accuracy (entrenamiento vs validaciÃ³n)** para los mismos modelos.

Puntos a observar en las grÃ¡ficas:

- El **modelo lineal**:
  - Tiende a estabilizar su accuracy antes, pero en un **nivel mÃ¡s bajo**.
  - Sus curvas suelen ser mÃ¡s â€œsuavesâ€ pero menos expresivas.
- El **MLP con ReLU**:
  - Muestra una **mejora mÃ¡s rÃ¡pida** en las primeras Ã©pocas.
  - Llega a una **meseta de accuracy** mÃ¡s alta.
  - Puede aparecer un **gap trainâ€“val** si el modelo es muy grande (indicio de overfitting).

---

# ğŸ› ï¸ **Preprocesamiento y â€œFeature Engineeringâ€ para MNIST**

Aunque MNIST no tiene â€œfeatures tabularesâ€ como Titanic, sÃ­ se aplican varias decisiones importantes de preprocesamiento:

| TÃ©cnica / Paso                          | DescripciÃ³n                                                                                                        |
|----------------------------------------|--------------------------------------------------------------------------------------------------------------------|
| **NormalizaciÃ³n de pÃ­xeles**          | ConversiÃ³n de `uint8` a `float32` y reescalado a `[0, 1]` dividiendo entre 255.                                    |
| **Aplanado de imÃ¡genes (flatten)**    | Transformar cada imagen de 28Ã—28 â†’ vector de tamaÃ±o 784 (`X.reshape(-1, 28*28)`) para modelos densos.              |
| **CodificaciÃ³n de etiquetas**         | Uso de etiquetas enteras `0â€“9` con `sparse_categorical_crossentropy` (no requiere one-hot en Keras).              |
| **Split entrenamiento / validaciÃ³n**  | Uso de `validation_split` sobre `X_train` para monitorear generalizaciÃ³n durante el entrenamiento.                |
| **PreparaciÃ³n para futuras extensiones** | Mantener tambiÃ©n versiÃ³n `(28, 28, 1)` para modelos convolucionales (CNN) en trabajos posteriores.               |

> ğŸ”­ **Extensiones posibles:**  
> - Aplicar **data augmentation** (rotaciones suaves, traslaciones, zoom) para hacer el modelo mÃ¡s robusto.  
> - Migrar a una **CNN** simple (Conv â†’ Pool â†’ Dense) para comparar contra los MLP densos.  
> - Probar otros datasets â€œtipo MNISTâ€ como Fashion-MNIST o EMNIST para mayor dificultad.

---

# ğŸ§© **DiscusiÃ³n y ReflexiÃ³n Final**

- **Â¿CuÃ¡ndo deja de ser suficiente una â€œrectaâ€ (modelo lineal)?**  
  Cuando la frontera de decisiÃ³n necesaria para separar las clases (aquÃ­, dÃ­gitos) es **altamente no lineal** en el espacio de pÃ­xeles. El modelo lineal sÃ³lo puede aprender fronteras lineales en el espacio 784-D, mientras que los dÃ­gitos presentan variaciones de forma, grosor y estilo de escritura.

- **Â¿QuÃ© tanto gana el MLP?**  
  - Gana en **capacidad de representaciÃ³n**: puede aproximar funciones mucho mÃ¡s complejas.
  - Gana en **accuracy**, especialmente en validaciÃ³n y test.
  - Permite aprovechar al mÃ¡ximo la informaciÃ³n de las imÃ¡genes, aunque a costa de **mÃ¡s parÃ¡metros** y ri
