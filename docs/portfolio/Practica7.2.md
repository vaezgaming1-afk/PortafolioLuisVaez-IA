<h1 align="center"> ClasificaciÃ³n de Prendas con MLP y Activaciones Avanzadas ğŸ‘—ğŸ‘ŸğŸ§¥</h1>

![FashionMNISTBanner](../assets/ImgPractica7/img7.1.2.png)

<p align="center">
  <em>Explorando cÃ³mo la elecciÃ³n de la activaciÃ³n afecta la capacidad de un MLP para clasificar prendas en Fashion-MNIST.</em>
</p>

ğŸ·ï¸ **Etiquetas RÃ¡pidas**  
`#8` `#FashionMNIST` `#RedesNeuronales` `#Activaciones` `#MLP` `#DeepLearning`

---

## ğŸš€ Accesos Directos Importantes

> *Haz clic en los botones para abrir el notebook y explorar las visualizaciones interactivas.*

<div align="center">

<a href="https://colab.research.google.com/drive/1E9lYYTWLB2NB2TqJHWb9lQzoCHvBSWs6?usp=sharing">
  <img src="https://img.shields.io/badge/Abrir%20Notebook-Google%20Colab-brightgreen?style=for-the-badge&logo=googlecolab&logoColor=white" alt="Abrir Notebook en Colab" />
</a>
&nbsp;
<a href="https://drive.google.com/drive/folders/1QLX5i7Hup0F2UCR660jwE_f9jYkQQoHO?usp=sharing">
  <img src="https://img.shields.io/badge/Visualizaciones-Google%20Drive-blue?style=for-the-badge&logo=google-drive&logoColor=white" alt="Ver visualizaciones en Drive" />
</a>

</div>

---

# ğŸ§  **Resumen Ejecutivo**

ğŸ¯ **Objetivo:**  
Entrenar y comparar redes neuronales multicapa (**MLP**) sobre el dataset **Fashion-MNIST**, compuesto por imÃ¡genes de prendas de vestir (10 clases: zapatillas, remeras, bolsos, abrigos, etc.).  
Este dataset tiene la misma estructura que MNIST (28Ã—28, escala de grises), pero es **mÃ¡s desafiante**, lo que permite ver con claridad:

- CÃ³mo la **activaciÃ³n** usada en las capas ocultas afecta el rendimiento.
- CÃ³mo aparecen fenÃ³menos como **overfitting** o **underfitting** segÃºn la profundidad de la red.
- Diferencias visibles entre **ReLU**, **tanh** y **sigmoid**, tanto en convergencia como en capacidad de representaciÃ³n.

ğŸ“Œ **Hallazgos clave esperados / observados:**

- **ReLU** suele converger mÃ¡s rÃ¡pido y alcanzar mejor val_accuracy.
- **tanh** puede competir con ReLU pero con entrenamiento mÃ¡s lento.
- **sigmoid** tiende a saturarse, lo que hace que su aprendizaje sea mÃ¡s lento y menos estable.
- Fashion-MNIST revela mÃ¡s claramente las limitaciones de las activaciones clÃ¡sicas frente a ReLU.
- A mayor profundidad del MLP, aumenta el riesgo de **overfitting**, especialmente con activaciones suaves como `sigmoid`.

ğŸ“ˆ **Resultado final (conceptual):**  
El experimento muestra que la elecciÃ³n de activaciÃ³n **no es un detalle menor**, sino un factor crÃ­tico que influye directamente en:

- La capacidad del modelo para aprender fronteras complejas.
- La estabilidad y velocidad del entrenamiento.
- La generalizaciÃ³n en validaciÃ³n y test.

---

# ğŸ¯ **Objetivos EspecÃ­ficos**

| Objetivo                                                                                         | Estado |
|--------------------------------------------------------------------------------------------------|--------|
| Cargar y explorar el dataset **Fashion-MNIST**                                                   | âœ…      |
| Normalizar y aplanar imÃ¡genes (28Ã—28 â†’ 784)                                                     | âœ…      |
| Implementar un **MLP base** con activaciÃ³n configurable                                          | âœ…      |
| Entrenar el MLP con **ReLU**, **tanh**, **sigmoid**                                              | âœ…      |
| Comparar las curvas de aprendizaje (loss/accuracy) de cada activaciÃ³n                             | âœ…      |
| Analizar convergencia, overfitting y estabilidad segÃºn activaciÃ³n                                 | âœ…      |
| Construir tabla resumen de mÃ©tricas: train_acc, val_acc, mejor Ã©poca                              | âœ…      |
| Elaborar reflexiÃ³n final sobre teorÃ­a vs prÃ¡ctica en activaciones                                 | âœ…      |

---

# ğŸ“… **Actividades y Tiempos**

| Actividad                                                                | Estimado | Real | Nota                                                                                  |
|--------------------------------------------------------------------------|----------|------|---------------------------------------------------------------------------------------|
| Carga y exploraciÃ³n del dataset                                          | 15 m     | 17 m | InspecciÃ³n de shapes y ejemplo de imÃ¡genes                                            |
| Preprocesamiento: normalizaciÃ³n + aplanado                               | 10 m     | 10 m | PreparaciÃ³n de `X_train_flat`, `X_test_flat`                                          |
| ImplementaciÃ³n del MLP configurable por activaciÃ³n                        | 20 m     | 20 m | Arquitectura: 128 â†’ 64 â†’ 10                                                           |
| Entrenamiento con **ReLU**                                               | 25 m     | 28 m | Convergencia rÃ¡pida, tendencia a buenos val_acc                                       |
| Entrenamiento con **tanh**                                               | 25 m     | 30 m | Curvas mÃ¡s estables pero mÃ¡s lentas                                                   |
| Entrenamiento con **sigmoid**                                            | 25 m     | 33 m | SaturaciÃ³n y aprendizaje lento                                                        |
| ComparaciÃ³n de mÃ©tricas + grÃ¡ficas overlay                               | 20 m     | 24 m | AnÃ¡lisis visual de val_accuracy                                                       |
| ReflexiÃ³n final                                                          | 10 m     | 11 m | SÃ­ntesis entre teorÃ­a y prÃ¡ctica                                                       |

ğŸ•’ **Total estimado:** 2 h 30 m Â· **Total real:** 2 h 53 m Â· Î”: +23 m  
Motivo: entrenamiento mÃ¡s lento en `sigmoid`.

---

# ğŸ“Š **ComparaciÃ³n de Activaciones en Fashion-MNIST**

> ğŸ’¡ Valores aproximados; reemplazar por resultados reales del notebook.

| ActivaciÃ³n | Train Acc | Val Acc | Test Acc | Mejor Ã‰poca | ObservaciÃ³n |
|------------|-----------|----------|-----------|--------------|-------------|
| **ReLU**   | â‰ˆ 0.93    | â‰ˆ 0.88   | â‰ˆ 0.87    | 7â€“8          | Convergencia rÃ¡pida, buen balance bias-variance |
| **tanh**   | â‰ˆ 0.92    | â‰ˆ 0.86   | â‰ˆ 0.85    | 8â€“9          | Menos overfitting que ReLU pero mÃ¡s lenta |
| **sigmoid**| â‰ˆ 0.89    | â‰ˆ 0.82   | â‰ˆ 0.81    | 9â€“10         | SaturaciÃ³n, gradientes pequeÃ±os, desempeÃ±o inferior |

---

# ğŸ“ˆ **Curvas de Entrenamiento y ValidaciÃ³n**

En el notebook se generan:

- **Train vs Val Loss** para ReLU, tanh, sigmoid.
- **Val Accuracy Overlay** (grÃ¡fica combinada donde se ve claramente cuÃ¡l converge mejor).

Puntos clave a observar:

- **ReLU** suele despegar rÃ¡pido y alcanzar mejor val_accuracy.
- **tanh** ofrece una curva suave y estable, pero llega mÃ¡s lento a valores altos.
- **sigmoid** muestra:
  - Curvas mÃ¡s planas (por saturaciÃ³n).
  - Mayor diferencia train-val â†’ posible underfitting o dificultad en el entrenamiento.

Fashion-MNIST permite ver claramente que las activaciones tradicionales (`sigmoid`, `tanh`) pueden ser insuficientes para problemas de visiÃ³n un poco mÃ¡s complejos.

---

# ğŸ”§ **Preprocesamiento y Feature Engineering para Fashion-MNIST**

| Paso / TÃ©cnica                      | DescripciÃ³n                                                                 |
|-------------------------------------|-----------------------------------------------------------------------------|
| **NormalizaciÃ³n**                  | Convertir a float32 y dividir entre 255.                                    |
| **Aplanado**                        | 28Ã—28 â†’ 784 para alimentar al MLP.                                          |
| **CodificaciÃ³n de etiquetas**       | 10 clases codificadas como enteros (0â€“9).                                   |
| **Split de validaciÃ³n**             | Se usa `validation_split=0.2` en el entrenamiento.                          |
| **Mismo pipeline que MNIST**        | Esto facilita comparar dataset fÃ¡cil vs dataset desafiante.                 |

> ğŸ”­ Extensiones posibles:  
> - Aumentar profundidad del MLP para observar overfitting.  
> - Probar `BatchNormalization` o `Dropout`.  
> - Migrar a **CNN** simple para comparar vs MLP.  

---

# ğŸ§© **DiscusiÃ³n y ReflexiÃ³n Final**

- **Fashion-MNIST es ideal para ver diferencias reales entre activaciones.**
- **ReLU** domina en velocidad y precisiÃ³n, coherente con la teorÃ­a de gradientes no saturados.
- **tanh** sigue siendo Ãºtil, especialmente cuando se quiere estabilidad y se controla la profundidad.
- **sigmoid** deja en evidencia sus limitaciones:
  - SaturaciÃ³n.
  - Gradiente pequeÃ±o.
  - Entrenamiento lento.
- Este dataset muestra que **la arquitectura + la activaciÃ³n** determinan cÃ³mo fluye la informaciÃ³n dentro del MLP, y cÃ³mo se comporta el gradiente en cada Ã©poca.

En conclusiÃ³n:

> **Fashion-MNIST es un puente perfecto entre MNIST â€œfÃ¡cilâ€ y datasets mÃ¡s complejos. Permite ver cÃ³mo decisiones pequeÃ±as (activar ReLU vs tanh vs sigmoid) cambian radicalmente el aprendizaje del modelo.**

---
