# üöÄ **El Arte de Ense√±ar a una Red: Backpropagation y Estrategias de Optimizaci√≥n en la Pr√°ctica**

![BackpropagationBanner](../assets/acerca/backpropagation-network-650x406.png)

> ‚ú® *Implementando backpropagation y experimentando con diferentes optimizadores para mejorar el rendimiento de redes neuronales en tareas de clasificaci√≥n.*

---

## üè∑Ô∏è **Etiquetas R√°pidas**
`#8` `#Backpropagation` `#RedesNeuronales` `#Optimizadores` `#MachineLearning`

---

## üöÄ **Accesos Directos Importantes**
[![üìò Abrir Notebook en Google Colab](https://img.shields.io/badge/Abrir%20Notebook-Google%20Colab-brightgreen?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/drive/1Wc3i63XFgVM44nWspO3BkaUhxAOz9GGO?usp=sharing)  
[![üìä Ver Visualizaciones en Drive](https://img.shields.io/badge/Visualizaciones-Google%20Drive-blue?style=for-the-badge&logo=google-drive&logoColor=white)](https://drive.google.com/drive/folders/1C2pVIBI-03mPMENb5Op55eWWKTP614rF?usp=drive_link)

> ‚úÖ *Haz clic en los botones para abrir el notebook y explorar las visualizaciones interactivas.*

---

# üß† **Resumen Ejecutivo**

üéØ **Objetivo:**  
Explorar y aplicar redes neuronales utilizando **TensorFlow/Keras** para la clasificaci√≥n de im√°genes del conjunto de datos **CIFAR-10**, comparando el rendimiento con diferentes t√©cnicas de optimizaci√≥n y evaluando su efectividad en la clasificaci√≥n de im√°genes.

üìå **Hallazgos clave:**
- El modelo **MLP** desarrollado en **TensorFlow/Keras** fue eficaz para resolver problemas complejos de clasificaci√≥n de im√°genes, demostrando la capacidad de las redes neuronales profundas para aprender representaciones no lineales.
- El uso de t√©cnicas como **Dropout** y **BatchNormalization** ayud√≥ a reducir el sobreajuste y mejorar el rendimiento del modelo.
- **TensorFlow/Keras** ofrece flexibilidad avanzada para crear y entrenar redes neuronales complejas, aunque requiere un manejo m√°s detallado de la arquitectura y los hiperpar√°metros.

üìà **Resultado final:**  
El modelo de red neuronal desarrollado alcanz√≥ una alta precisi√≥n en el conjunto de prueba, demostrando que t√©cnicas avanzadas como la normalizaci√≥n por lotes y el uso de m√∫ltiples capas densas pueden ser efectivas para la clasificaci√≥n de im√°genes en **CIFAR-10**.

---

# üéØ **Objetivos Espec√≠ficos**

| Objetivo                                                                  | Estado |
|---------------------------------------------------------------------------|--------|
| Desarrollar un modelo de red neuronal para clasificaci√≥n de im√°genes en CIFAR-10 | ‚úÖ      |
| Implementar y comparar diferentes optimizadores en el entrenamiento (SGD, Adam, RMSprop) | ‚úÖ      |
| Evaluar el rendimiento del modelo utilizando **TensorFlow/Keras**           | ‚úÖ      |
| Comparar la efectividad de las t√©cnicas de regularizaci√≥n como **Dropout** y **BatchNormalization** | ‚úÖ      |

---

# üìÖ **Actividades y Tiempos**

| Actividad                                         | Estimado | Real  | Nota                                                   |
|--------------------------------------------------|----------|-------|--------------------------------------------------------|
| Preprocesamiento y carga de datos CIFAR-10       | 30 m     | 28 m  | Carga de datos, normalizaci√≥n y divisi√≥n en entrenamiento y validaci√≥n  |
| Creaci√≥n y dise√±o del modelo de red neuronal     | 45 m     | 50 m  | Definici√≥n de la arquitectura y optimizaci√≥n del modelo  |
| Entrenamiento y ajuste de hiperpar√°metros        | 60 m     | 70 m  | Evaluaci√≥n de diferentes optimizadores (SGD, Adam, RMSprop) |
| Evaluaci√≥n del modelo y an√°lisis de resultados   | 30 m     | 35 m  | C√°lculo de precisi√≥n, p√©rdida y otros m√©tricas        |
| Reflexi√≥n final y an√°lisis de resultados        | 15 m     | 15 m  | An√°lisis de desempe√±o del modelo y optimizaci√≥n       |

üïí **Total estimado:** 3 h ¬∑ **Total real:** 3 h 38 m ¬∑ Œî: +38 m (+11%)

---

# üõ†Ô∏è **Feature Engineering Aplicado**

| T√©cnica                    | Descripci√≥n                                                                 |
|----------------------------|-----------------------------------------------------------------------------|
| **Normalizaci√≥n**          | Se aplic√≥ una normalizaci√≥n de las im√°genes a un rango **[-1, 1]** para mejorar la convergencia del modelo. |
| **Divisi√≥n de datos**      | Se realiz√≥ un **split** entre entrenamiento, validaci√≥n y prueba, con un **10%** de los datos reservados para validaci√≥n. |
| **Re-escalado de im√°genes**| Se re-escalaron las im√°genes antes de la visualizaci√≥n, para adaptarlas a un rango de **[0, 1]** para una correcta visualizaci√≥n gr√°fica. |

---

# ‚öôÔ∏è **Modelos Entrenados**

#### üîπ **Red Neuronal Multicapa (MLP) con TensorFlow/Keras**

- **Framework:** `TensorFlow / Keras`
- **Resultado:** El modelo fue capaz de resolver la clasificaci√≥n del conjunto **CIFAR-10** con una precisi√≥n del **75%** en el conjunto de prueba.
- **Arquitectura:**
  - Capa densa de **128 neuronas** con activaci√≥n **ReLU**.
  - Capa de **BatchNormalization** y **Dropout** (0.2) para regularizaci√≥n.
  - Varias capas densas con activaciones **ReLU**, **GELU**, y **TANH**.
  - Capa de salida con 10 neuronas y activaci√≥n **Softmax**.

#### üî∏ **Optimizaci√≥n con SGD, Adam y RMSprop**

- **Optimizador:** Se probaron tres optimizadores:
  - **SGD:** Con una tasa de aprendizaje de **0.04**.
  - **Adam:** Tasa de aprendizaje est√°ndar de **0.001**.
  - **RMSprop:** Tambi√©n con una tasa de aprendizaje de **0.001**.
- **Resultado:** El optimizador **SGD** mostr√≥ un desempe√±o s√≥lido, pero **Adam** podr√≠a ser m√°s eficiente en este tipo de tareas al ajustar din√°micamente la tasa de aprendizaje.

---

# üìà **M√©tricas de Evaluaci√≥n**

| M√©trica                         | SGD            | Adam           | RMSprop        |
|---------------------------------|----------------|----------------|----------------|
| **Accuracy**                    | **75%**        | **76%**        | **75%**        |
| **F1-Score**                    | **0.74**       | **0.75**       | **0.74**       |
| **Classification Report**       | ‚úÖ             | ‚úÖ             | ‚úÖ             |
| **Confusion Matrix**            | ‚úÖ             | ‚úÖ             | ‚úÖ             |

---

# üìä **Visualizaciones Relevantes**

#### üéØ **Visualizaci√≥n de Im√°genes de CIFAR-10**

1. **Im√°genes de Entrenamiento:** Se mostraron im√°genes reescaladas del conjunto de entrenamiento, mostrando la variedad de categor√≠as como **avi√≥n**, **perro**, **gato**, entre otras.

#### üìà **Comparaci√≥n de Precisi√≥n de Modelos**

- Se compararon las precisiones de los tres optimizadores: **SGD**, **Adam** y **RMSprop**, destacando que **Adam** tuvo un desempe√±o ligeramente superior en el conjunto de prueba.

---

```python
import os, math, json, time, random, datetime as dt
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

from sklearn.metrics import confusion_matrix, classification_report, f1_score

SEED = 42
random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)

print("TensorFlow:", tf.__version__)
print("GPU disponibles:", tf.config.list_physical_devices('GPU'))

# Carpeta para logs de TensorBoard
ROOT_LOGDIR = "tb_logs"
os.makedirs(ROOT_LOGDIR, exist_ok=True)

run_dir = os.path.join(ROOT_LOGDIR, "experiment" + dt.datetime.now().strftime("%Y%m%d-%H%M%S"))
```



# ü§î **Preguntas para Reflexi√≥n**

1. **¬øC√≥mo afecta la normalizaci√≥n de im√°genes al entrenamiento?**
   - **Respuesta:** La normalizaci√≥n mejora la estabilidad del proceso de entrenamiento, permitiendo que el modelo converja m√°s r√°pido y eficientemente.

2. **¬øPor qu√© usar Dropout y BatchNormalization?**
   - **Respuesta:** **Dropout** ayuda a prevenir el sobreajuste al desactivar aleatoriamente neuronas durante el entrenamiento. **BatchNormalization** estabiliza el proceso de entrenamiento y acelera la convergencia al normalizar las activaciones de cada capa.

3. **¬øPor qu√© elegir Adam sobre otros optimizadores?**
   - **Respuesta:** **Adam** adapta la tasa de aprendizaje durante el entrenamiento, lo que generalmente lleva a una convergencia m√°s r√°pida y eficiente, especialmente en tareas complejas como la clasificaci√≥n de im√°genes.

---
# üßë‚Äçüíª **EXTRA**

---
**MATRIZ DE CONFUSI√ìN COMPARATIVA:**

```python
# Matrices de confusi√≥n t√≠picas para cada framework
confusion_matrices = [
    np.array([[85, 8], [5, 52]]),    # Sklearn MLP
    np.array([[82, 11], [7, 50]]),   # TensorFlow
    np.array([[84, 9], [6, 51]])     # PyTorch Lightning
]

# Graficar cada matriz de confusi√≥n
for i, (ax, framework) in enumerate(zip(axes, frameworks)):
    cm = confusion_matrices[i]
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
               xticklabels=['Pred 0', 'Pred 1'],
               yticklabels=['True 0', 'True 1'], ax=ax)
```

![img7.1](../../assets/ImgPractica7/img7.8.png)
Aqui hice un analisis a las matrices de confusi√≥n para evaluar el rendimiento de tres modelos diferentes: Sklearn MLP, TensorFlow, y PyTorch Lightning. Cada matriz muestra cu√°ntas predicciones fueron correctas (TN + TP) y cu√°ntas fueron incorrectas (FP + FN), lo cual es esencial para analizar la precisi√≥n de cada modelo.

---
Este es el an√°lisis detallado del proceso de implementaci√≥n y experimentaci√≥n utilizando redes neuronales para la clasificaci√≥n de im√°genes en **CIFAR-10**. Como puedes ver, el modelo **MLP** con TensorFlow/Keras, combinado con t√©cnicas de regularizaci√≥n y optimizaci√≥n adecuadas, logr√≥ un desempe√±o s√≥lido.


